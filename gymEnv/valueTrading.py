from gym import Env
from gym import spaces
import numpy as np
import pandas as pd
from dateutil.relativedelta import relativedelta
import config

class valueTradingEnv(Env):
    """
    Inherits from gym.Env. It is a stock trading Environment

    params:
        - df (pandas DataFrame): DF with 'date' and 'symbol' set as multilevel index and at least open and closing prices as first and second column. IMPORTANT: open and Closing price always has to be as first and second columns respectively for taking actions and calculating reward.
        - train (bool): Are we building the Env for training or not? It is used for calculation the daterante in DFs that are equal size of daterange.
        - yearrange (int): How many years will one episode take? DF has to be at least this range.
    returns:
        A gym.Env object.
    """
    metadata = {'render.modes': "human"}
    
    def __init__(self, df: pd.DataFrame, train: bool, yearrange: int=4):
        # self variables
        self.df = df
        self.train = train
        self.yearrange = yearrange
        self.df_dt_filter = self.df.index.get_level_values(level="date")
        self.indicators = self.df.columns.tolist()
        self.num_symbols = len(self.df.index.get_level_values(level="symbol").unique().tolist())
        self.fee = config.TRADE_FEE_PRCT
        self.scaling = config.ACTION_SCALING
        self.init_cash = config.INIT_CASH

        # Vars not yet set
        self.done = None
        self.reward = None
        self.state = None
        self.new_state = None
        self.info = None
        self.cost = None
        self.date = None
        self.date_idx = None
        self.end_date = None
        self.data = None
        self.data_dt_filter = None
        self.data_dt_unique = None

        # Spaces
        self.action_space = spaces.Box(low = -1, high = 1,shape = (self.num_symbols,))
        obs_shape = 1 + self.num_symbols + (len(self.indicators) * self.num_symbols)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape = (obs_shape,))

    def _sell_stock(self, num, index):
        # Are we selling less or equal num of stocks we have?
        if self.new_state[1+index] >= num:
            # get price of stock to calculate amount
            price = self.new_state[1+self.num_symbols+index]
            amount = price * num
            # calculate cost
            self.cost[index] = amount * self.fee
            # put the stock from portfolio
            self.new_state[1+index] -= num
            # recalculate the cash
            self.new_state[0] += (amount - self.cost[index])
        else:
            pass
    
    def _buy_stock(self, num, index):
        # get price of stock
        price = self.new_state[1+self.num_symbols+index]
        amount = price * num
        # calculate cost
        self.cost[index] = amount * self.fee

        # Check if we have enough cash
        if self.new_state[0] >= amount+self.cost[index]:
            # call the stock into portfolio
            self.new_state[1+index] += num
            # update the cash
            self.new_state[0] -= (amount + self.cost[index])
        else:
            pass

    def _get_time_range(self):
        # get all unique dates in df
        dates = self.df_dt_filter.unique()
        if self.train:
            # set max end date to 4 years befor max date
            max_end = dates.max() - relativedelta(years=self.yearrange)
            min_begin = dates.min()
            # throw away all dates out of begin and end
            dates = dates[dates.slice_indexer(min_begin, max_end)].tolist()
            # sample start date randomly out of possible dates
            start_date = np.random.choice(dates)
            # set end date 4yrs-1day relative to start date
            end_date = start_date + relativedelta(years=4,days=-1)
        else: # If we are not in train environment
            # Set start date and end date to min and max of df respectively
            start_date = dates.min()
            end_date = dates.max()
        
        return (start_date, end_date)

    def step(self, action):
        """
        Run one timestep of the environment's dynamics.

        Accepts an action and returns a tuple (observation, reward, done, info).

        Args:
            action (object): an action provided by the agent

        Returns:
            observation (object): agent's observation of the current environment
            reward (float) : amount of reward returned after previous action
            done (bool): whether the episode has ended, in which case further step() calls will return undefined results
            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)
        """
        self.reward = 0
        self.cost = [0]*self.num_symbols
        # Skale action by item
        action = np.array([int(item*self.scaling) for item in action])
        real_action = action # save for info

        # count date one day ahead
        self.date_idx += 1
        self.date = self.data_dt_unique[self.date_idx]
        # set up new state based on current state
        # We manipulate the portfolios cash and number of shares in new_state when buying and selling
        # we need the old portfolio balance to calculate the reward
        self.new_state =    [self.state[0]] + \
                            self.state[1:(1+self.num_symbols)] + \
                            [item for indicator in self.indicators for item in self.data[self.data_dt_filter == self.date][indicator].values.tolist()]
        
        # Set action of stock where open, close, high and low is 0 to 0
        for idx in range(len(action)):
            prc_open = self.new_state[1+self.num_symbols+idx]
            prc_close = self.new_state[1+self.num_symbols*2+idx]
            prc_high = self.new_state[1+self.num_symbols*3+idx]
            prc_low = self.new_state[1+self.num_symbols*4+idx]
            if prc_open == 0 and prc_close == 0 and prc_high == 0 and prc_low == 0:
                action[idx] = 0
        # Sort actions from lowest to highest
        argsort_actions = np.argsort(action)
        # get indices of sell actions
        sell_indices = argsort_actions[:np.where(action < 0)[0].shape[0]]
        # get indices of buy actions
        buy_indices = argsort_actions[::-1][:np.where(action > 0)[0].shape[0]]

        # perform each sell action
        for idx in sell_indices:
            self._sell_stock(action[idx]*-1, idx)
        # perform each buy action
        for idx in buy_indices:
            self._buy_stock(action[idx], idx)

        # calculate reward
        new_total_amount = self.new_state[0] + \
                sum(np.array(self.new_state[1:(1+self.num_symbols)]) * \
                    np.array(self.new_state[(1+self.num_symbols*2):(1+self.num_symbols*3)]))
        old_total_amount = self.state[0] + \
                sum(np.array(self.state[1:(1+self.num_symbols)]) * \
                    np.array(self.state[(1+self.num_symbols*2):(1+self.num_symbols*3)]))
        self.reward = new_total_amount - old_total_amount

        # set new_state as current state
        self.state = self.new_state

        # add the values to the info container
        self.info["dates"].append(self.date)
        self.info["actions"].append(action)
        self.info["realActions"].append(real_action)
        self.info["rewards"].append(self.reward)
        self.info["cashes"].append(self.state[0])
        self.info["numShares"].append(self.state[1:(1+self.num_symbols)])
        self.info["openPrices"].append(self.state[(1+self.num_symbols):(1+self.num_symbols*2)])
        self.info["closePrices"].append(self.state[(1+self.num_symbols*2):(1+self.num_symbols*3)])
        self.info["costs"].append(self.cost)

        # Check done conditions
        # Is date equal to end_date?
        if self.date == self.end_date:
            self.done = True
        # Is the cash lower than x% of init_cash?
        # If we set this to 0.0 we allow to don't hold cash anyway
        if self.state[0] < self.init_cash*0.1:
            self.done = True

        # This is because the agent would not do another step if de env is done
        # So we need to append some zeros to make the lists the identical lengths
        if self.done:
            self.info["actions"].append([0]*len(action))
            self.info["realActions"].append([0]*len(real_action))
            self.info["rewards"].append(0)
            self.info["costs"].append(0)
        
        return (self.state, self.reward, self.done, self.info)

    def reset(self):
        """Resets the environment to an initial state and returns an initial
        observation.

        Note that this function should not reset the environment's random
        number generator(s); random variables in the environment's state should
        be sampled independently between multiple calls to `reset()`. In other
        words, each call of `reset()` should yield an environment suitable for
        a new episode, independent of previous episodes.

        Returns:
            observation (object): the initial observation.
        """
        # Get date range
        start_date, end_date = self._get_time_range()
        # slice episode data out of df
        self.data = self.df[(self.df_dt_filter >= start_date) & (self.df_dt_filter <= end_date)]
        # get a filter object out of index level date
        self.data_dt_filter = self.data.index.get_level_values(level="date")
        self.data_dt_unique =  self.data_dt_filter.unique().tolist()
        # Reset date_idx
        self.date_idx = 0
        # get first date object
        self.date =self.data_dt_unique[self.date_idx]
        # set real end date
        self.end_date = self.data_dt_unique[-1]
        self.done = False
        
        # generate first state
        self.state =    [self.init_cash] + \
                        [0]*self.num_symbols + \
                        [item for indicator in self.indicators for item in self.data[self.data_dt_filter == self.date][indicator].values.tolist()]
        
        # info container for rendering and output
        self.info = {
            "dates": [self.date],
            "actions": [],
            "realActions": [],
            "rewards": [],
            "cashes": [self.state[0]],
            "numShares": [self.state[1:(1+self.num_symbols)]],
            "openPrices": [self.state[(1+self.num_symbols):(1+self.num_symbols*2)]],
            "closePrices": [self.state[(1+self.num_symbols*2):(1+self.num_symbols*3)]],
            "costs": []
        }

        return self.state
    
    def render(self, mode='human'):
        """
        Prints or plots some basic information.

        Args:
            mode (str): the mode to render with
        """
        if mode == 'human':
            print("Here should be a basic plot...")
        else:
            super(valueTradingEnv, self).render(mode=mode) # just raise an exception

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]
